#! /usr/bin/perl

use strict;
use warnings;
use v5.16;

# might be assumed to be installed on any perl system by default?
use Data::Dumper;
use English qw( -no_match_vars );
use File::Basename;
use POSIX;
use Fcntl qw(:DEFAULT :flock); # import LOCK_* constants
use File::Temp qw/ tempfile /;
use Env qw( HOME HOSTNAME SHORTNAME );
use Pod::Usage;
eval { use Time::HiRes qw( time ); };
use File::Spec;
use Date::Parse;
use Storable qw(lock_store lock_retrieve);

# added from CPAN or system packages
use Log::Dispatch;
use Log::Dispatch::Syslog;
use Log::Dispatch::Screen;
use Log::Dispatch::File;
use AppConfig qw(:expand :argcount);
use Getopt::Long qw(:config pass_through) ; # use this to pull out the config file, then let AppConfig handle the rest

##### GLOBALS ##### # the horrror:
# use this string when refering to ourself:
my $APP_NAME = 'delta-dumper';

# tease the user out of the environment, or just call getpwuid:
my $USER
    = $ENV{LOGNAME} || $ENV{USERNAME} || $ENV{USER} || scalar( getpwuid($<) );

# define what programs to use for what compression setting:
my %compressors = (
    'gzip' => {
        'compress'   => 'gzip -c --rsyncable',
        'uncompress' => 'gzip -c -d',
        'extension'  => 'gz',
    },
    'bzip2' => {
        'compress'   => 'bzip2 -c',
        'uncompress' => 'bzcat',
        'extension'  => 'bz2'
    },
    'xz' => {
        'compress'   => 'xz -T0 --stdout',
        'uncompress' => 'xzcat -T0',
        'extension'  => 'xz'
    },
    'zstd' => {
        'compress'   => 'zstd -T0 --stdout',
        'uncompress' => 'zstd -T0 --uncompress --stdout',
        'extension'  => 'zst',
    },
    'compress' => {
        'compress'   => 'compress -c',
        'uncompress' => 'uncompress -c',
        'extension'  => 'Z',
    }
);

# translate lowercase database types to funky case
my %databasetype_case = (
    'mysql'      => 'MySQL',
    'postgresql' => 'PostgreSQL',
    'mongodb'    => 'MongoDB',
);

# list of readonly periods:
my @periods = qw( daily weekly monthly );

# global app configuration
my $config;

# global status variable for writing to, used by monitoring
my $status; # it's a hashref

# the path to the config file to be parsed
my $CONFIG_FILE
    = $USER eq 'root'
    ? File::Spec->catfile( '/etc', $APP_NAME, 'config' )
    : File::Spec->catfile( $HOME, '.config', $APP_NAME, 'config' );

# the path to the status file
my $STATUS_FILE
    = $USER eq 'root'
    ? File::Spec->catfile( '/etc', $APP_NAME, 'status' )
    : File::Spec->catfile( $HOME, '.config', $APP_NAME, 'status' );

# unused, unloved, restore mode!
my $RESTORE=0;

# create the call back for the dispatcher at the top level (probably
# not needed)
my $callback_clean = sub { my %t=@_;
                           chomp $t{message};
                           return $t{message}."\n"; # add a newline
                         };

# global dispatcher variable for use in dlog() etc.
my $DISPATCHER;

# create a logparams for logging, intended to be localized:
our %logparams;

# just ensure hostname is set to something
unless($HOSTNAME and length $HOSTNAME > 0) {
    $HOSTNAME=`hostname`;
    chomp $HOSTNAME;
}
unless($SHORTNAME and length $SHORTNAME > 0) {
    $SHORTNAME=$HOSTNAME;
    chomp $SHORTNAME;
}

if ( scalar split(/\./, $SHORTNAME) > 1 ) {
    my @a = split(/\./, $SHORTNAME);
    $SHORTNAME=shift @a;
}

######## MAIN ########

main();

######## SUBROUTINES ########
sub lock_file_compose {
  return sprintf('%s/%s.lock',$config->backup_location,$_[0]);
}
sub lock_pid_file {
  my $MAXWAIT=30;
  my $LOCK_FILE=lock_file_compose(@_);
  my $LOCK;
  my $waittime=time();
  my $locked=0;

  unless(open($LOCK,'+<',$LOCK_FILE) or open($LOCK,'>',$LOCK_FILE)) {
    return 0;                   # false or fail
  }
  eval {
    local $SIG{ALRM} = sub { die "alarm\n" }; # NB: \n required
    alarm $MAXWAIT;
    if (flock($LOCK,LOCK_EX)) {
      $locked=1;
    }
    alarm 0;
  };
  if ($@) {
    die unless $@ eq "alarm\n"; # propagate unexpected errors
  } else {
    if ($locked) {
      truncate($LOCK,0); # this shouldn't fail if we have the file opened and locked!
      print $LOCK $$."\n";      # who really cares if this fails?
      return $LOCK;             # happiness is a locked file
    }
  }
  # this is the fall through for the cases where we have received an alarm
  # or we failed to lock the file without receiving a signal
  close $LOCK;
  return 0;
}
sub unlock_pid_file {
  flock($_[0],LOCK_UN);
  close $_[0];
}

# create the dispatcher and return it
sub create_dispatcher {
  my $disp=Log::Dispatch->new( callbacks => $callback_clean );

  if ($config->sys_logging) {
    $disp->add(Log::Dispatch::Syslog->new(name      => 'syslog',
                                          min_level => $config->level,
                                          ident     => $APP_NAME.'['.$$.']',
                                          facility  => $config->facility,
                                          socket    => 'unix',
                                         )
              );
  }
  if ($config->terminal_logging) {
    $disp->add(Log::Dispatch::Screen->new(name      => 'screen',
                                          min_level => $config->log_level,
                                          stderr    => 1,
                                         )
              );
  }
  if ($config->file_logging) {
    $disp->add(
               Log::Dispatch::File->new(
                                        name      => 'logfile',
                                        min_level => $config->level,
                                        filename  => File::Spec->catfile($config->log_location,"${APP_NAME}.log-".strftime('%Y%m%d',localtime(time()))),
                                        mode      => '>>',
                                       )
              );
  }
  return $disp;
}

# string turns hash refs into a string of key value pairs, it does not recurse
sub stringy {
  # each element passed to stringy should be a HASH REF
  my %a; # strings
  my %specials  = (
      tag => 1,
      host => 1,
  );
  foreach my $h (@_) {
    next unless ref $h eq 'HASH';
    while( my ($key,$val) = each(%$h) ) {
      next if ref ${$h}{$key}; # must not be a reference
      $val =~ s/\n/NL/g; # remove newlines
      $val =~ s/"/\\"/g; # replace " with \"
      if ($key =~ /^pass/) {
        $val = 'XXXXXXXX';
      } elsif( $key eq 'databasetype' and defined $databasetype_case{$val}) {
        $val = $databasetype_case{$val};
      } elsif( $key eq 'runtime' or $key eq 'total_run_time_seconds' ){
          $val = sprintf("%.5f", $val);
      }
      if($specials{$key}) {
        $a{"${APP_NAME}_${key}"}=$val;
      } else {
        $a{$key}=$val;
      }
    }
  }
  my @f;
  foreach my $key (sort {&sort_tags} (keys(%a))) {
    push(@f,"${key}=\"$a{$key}\"");
  }
  return join(" ",@f);
}

# main logging function, using global dispatcher
sub dlog {
    my $level = shift;
    my $msg   = shift;
    my $time  = time();
    my $str   = stringy(
        {   'msg'      => $msg,
            'severity' => $level,
            timestamp  => $time,
            datetime   => strftime( "%FT%T%z", localtime($time) ),
            hostname   => $HOSTNAME,
            pid        => $$,
        },
        @_
    );
    $DISPATCHER->log(
        level   => $level,
        message => $str,
    );
    return $str;
}

# sort function, use the priority of a tag to compare,
# otherwise use string compare (alphabetical)
sub sort_tags {
    my $p = tag_prio($a) <=> tag_prio($b);
    if ( $p == 0 ) {
        return $a cmp $b;
    }
    return $p;
}

sub tag_prio {
  my %prios = (
      'datetime'  => -10,
      'hostname'  => -9,
      'severity'  => -8,
      'msg'       => -5,
      'timestamp' => 50,
  );
  my $t = lc $_[0];
  return $prios{$t} if defined $prios{$t};
  return 0;
}

sub build_mysql_exec {
  my $bin = shift;
  my @mysql_options = ($config->mysql_bindir ? File::Spec->catfile($config->mysql_bindir,$bin) : $bin);
  if ($config->mysql_defaults_file) {
    push(@mysql_options, '--defaults-file='.$config->mysql_defaults_file);
  }
  if($bin eq 'mysqldump') {
    if ($config->mysql_ignore_table and @{$config->mysql_ignore_table} > 0) {
      foreach my $t (@{$config->mysql_ignore_table}) {
        push(@mysql_options,"--ignore-table=${t}");
      }
    }
    if ($config->mysql_single_transaction) {
      push(@mysql_options, '--single-transaction');
    }
    if ($config->mysql_extra_option and scalar @{$config->mysql_extra_option} > 0) {
        push(@mysql_options, @{$config->mysql_extra_option});
    }
  }
  if ($config->mysql_user) {
    push(@mysql_options, '-u'.$config->mysql_user);
  }
  if ($config->mysql_hostname) {
    push(@mysql_options, '-h'.$config->mysql_hostname);
  }
  if ($config->mysql_password) {
    push(@mysql_options, '-p"'.$config->mysql_password.'"');
  }
  return @mysql_options;
}

sub build_mongodb_exec {
    my @options
        = ( $config->mysql_bindir
        ? File::Spec->catfile( $config->mysql_bindir, 'mongodump' )
        : 'mongodump' );
    push @options, '--archive';
    if ( $config->mongodb_username ) {
        push( @options, '--username', $config->mongodb_username );
    }
    if ( $config->mongodb_hostname ) {
        push( @options, '--hostname', $config->mongodb_hostname );
    }
    if ( $config->mongodb_password ) {
        push( @options, '--password', $config->mongodb_password );
    }
    if ( $config->mongodb_ssl ) {
        push( @options, '--ssl' );
    }
    if ( $config->mongodb_port ) {
        push( @options, '--port', $config->mongodb_port );
    }
    return @options;
}

sub mysql_database_list {
  my $mysql_com = join(' ',build_mysql_exec('mysql'));
  my $mysql_handle;
  my @dl;
  if (open $mysql_handle, '-|', "${mysql_com} -e \"SHOW DATABASES;\"") {
    my $c=0;
    while(<$mysql_handle>) {
      chomp $_;
      push(@dl,$_) unless $c == 0;
      $c++;
    }
  }
  else {
    dlog('critical','unable to launch mysql for listing databases',{return_code => POSIX::WEXITSTATUS($?)});
  }
  return @dl;
}

sub init_config {
    $config = AppConfig->new(
        { GLOBAL => { EXPAND => EXPAND_VAR | EXPAND_ENV, }, } );

    $config->define(
        'log_level' => {
            ALIAS    => "loglevel|level|log-level",
            DEFAULT  => 'info',
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
            VALIDATE =>
                '^(debug|info|notice|warning|error|critical|alert|emergency)$'
        },
        'facility' => {
            DEFAULT  => 'user',
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
            VALIDATE =>
                '^(auth|authpriv|cron|daemon|kern|local[0-7]|mail|news|syslog|user|uucp)$',
        },
        'mysql_skip_database' => {
            DEFAULT =>
                [ '#lost+found', 'performance_schema', 'information_schema' ],
            ARGCOUNT => ARGCOUNT_LIST,
            ARGS     => '=s@',
        },
        'postgresql_skip_database' => {
            DEFAULT  => ['#lost+found'],
            ARGCOUNT => ARGCOUNT_LIST,
            ARGS     => '=s@',
        },
        'compression' => {
            DEFAULT  => 'gzip',
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
            VALIDATE => '^(' . join( '|', keys(%compressors), 'custom', 'none' ). ')$',
        },
        'compress' => {
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
        },
        'uncompress' => {
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
        },
        'extension' => {
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
        },
        'backup_location=s' => {
            DEFAULT => '/var/delta-dumper/backups',
            VALIDATE => '^\/.+',    # require an absolute path?
        },
        'tmpdir' => {
            ALIAS    => 'tmp_dir|tempdir|temp_dir',
            ARGS     => '=s',
            VALIDATE => '^\/.+',       # require an absolute path?
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'dump_config' => {
            ALIAS    => 'dump-config',
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'dump_status' => {
            ALIAS    => 'dump-status',
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'log_location=s' => {
            DEFAULT => $USER eq 'root'
            ? '/var/log'
            : File::Spec->catfile( $HOME, 'var', 'log' ),
            VALIDATE => '^\/.+',    # require an absolute path?
        },
        'h|help!',
        'mysql_bindir' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\/.+',        # require an absolute path?
        },
        'postgresql_bindir' => {
            DEFAULT  => '/usr/bin',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\/.+',        # require an absolute path?
        },
        'postgresql_dump_database' => {
            ARGS     => '=s@',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'postgresql_extra_option' => {
            DEFAULT  => ['--clean','--if-exists'],
            ARGS     => '=s@',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'postgresql_username' => {
            DEFAULT  => 'postgres',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'postgresql_host' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql!',
        'postgresql!',
        'mongodb!',
        'localcow!',
        'sys_logging' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ALIAS    => 'syslog',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'terminal_logging' => {

            # if stderr is a terminal, enable terminal_loging
            DEFAULT  => -t STDERR ? 1 : 0,
            ARGS     => '!',
            ALIAS    => 't',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'file_logging' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'mysql_defaults_file' => {
            ALIAS    => 'mysql_defaults-file|defaults-file',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_single_transaction' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ALIAS    => 'single-transaction|mysql_single-transaction',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_ignore_table' => {
            DEFAULT  => ['mysql.events'],
            ARGS     => '=s@',
            ALIAS    => 'mysql_ignore-table|ignore-table',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'mysql_user' => {
            ARGS     => '=s',
            ALIAS    => 'mysql-user',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_password' => {
            ARGS     => '=s',
            ALIAS    => 'mysql-password',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_hostname' => {
            ARGS     => '=s',
            ALIAS    => 'mysql-hostname',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_extra_option' => {
            ARGS     => '=s@',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'daily' => {
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\d+$',
        },
        'weekly' => {
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\d+$',
        },
        'monthly' => {
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\d+$',
        },
        'week_start' => {
            DEFAULT  => 'Sun',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^(Sun|Mon|Tue|Wed|Thu|Fri|Sat)$',
        },
        'month_start' => {
            DEFAULT  => '1',
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^[0-9]+$',
        },
        'rsync_binary' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            DEFAULT  => 'rsync',
        },
        'rsync_options' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            DEFAULT  => '-a --inplace --no-whole-file',
        },
        'prerun' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'postrun' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_bindir' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\/.+',        # require an absolute path?
        },
        'mongodb_username' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_password' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_port' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_hostname' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_ssl' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
    );
}

sub main {
  init_config();
  parse_config();

  # set up our dispatch destinations:
  $DISPATCHER = create_dispatcher();

  ## Starting up:
  my $RUNTIME=time();
  dlog('info','starting',{});

  ## Dump the config if request, and exit:
  if ($config->dump_config) {
      print Dumper $config;
    die "configuration dump requested and delivered, exiting";
  }

  sanity_check_config();

  # obtain the global lock, or die trying
  my $GLOBAL_LOCK = lock_pid_file($APP_NAME);
  unless ( $GLOBAL_LOCK ) {
    my $emsg='unable to lock pid file: '.lock_file_compose($APP_NAME);
    dlog('warning',$emsg,{});
    die $emsg;
  }

  unless ( -d $config->backup_location ) {
    dlog('critical', 'backup location does not exist', { backup_location => $config->backup_location });
      die "output directory does not exist and WILL NOT be created: ".$config->backup_location;
  }
  # create sub-directories:
  foreach my $p (@periods,'current') {
    my $ddir = File::Spec->catfile($config->backup_location,$p);
    unless ( -d $ddir or mkdir($ddir)) {
      dlog('critical',
           "output directory does not exist and cannot be created: ${ddir}",
           { backup_location => $config->backup_location, ddir => $ddir });
      die "output directory does not exist and cannot be created: ${ddir}";
    }
  }
  if ( -f $STATUS_FILE ) {
      if ( $status = lock_retrieve($STATUS_FILE) and ref $status eq 'HASH' ) {
          dlog('debug',
               'retrieved status data',
               { storable => $STATUS_FILE });
      }
      else {
          dlog('warning',
               'unable to retrieve status data, or data returned not a hash',
               { storable => $STATUS_FILE });
          $status = {};
      }
  }
  else {
      dlog('debug',
           'status data does not exist, starting over',
           { storable => $STATUS_FILE });
      $status = {};
  }
  if ( $config->dump_status ) {
      print Dumper $status;
      die "dump request of status data, exiting";
  }
  if ( $config->prerun ) {
      unless( system_runner('prerun', $config->prerun) ) {
          unlock_pid_file($GLOBAL_LOCK);
          dlog('error',
               'exiting because prerun command failed',
               {'total_run_time_seconds' => time()-$RUNTIME});
          exit;
      }
  }

  if ($config->mysql) {
      foreach my $db (mysql_database_list()) {
          if ( $config->mysql_skip_database
               and string_any($db,@{$config->mysql_skip_database}) ) {
              dlog('notice',
                   'skipping database',
                   { databasetype => 'mysql', databasename => $db },
                  );
              next;
          }
          my $t = perform_backup('mysql',$db);
          my $s = 'success';
          unless ($t) {
              $s = 'failure';
              $t = time();
          }
          $$status{mysql}{$db}{$s} = $t;
      }
  }
  if ($config->postgresql) {
      foreach my $db (@{$config->postgresql_dump_database}) {
          my $t = perform_backup('postgresql',$db);
          my $s = 'success';
          unless ($t) {
              $s = 'failure';
              $t = time();
          }
          $$status{postgresql}{$db}{$s} = $t;
      }
  }
  if ($config->mongodb) {
      my $db = 'all';
      my $t = perform_backup('mongodb',$db);
      my $s = 'success';
      unless ($t) {
          $s = 'failure';
          $t = time();
      }
      $$status{mongodb}{$db}{$s} = $t;
  }

  if ( $config->postrun ) {
      system_runner('postrun', $config->postrun);
  }

  # release the lock explicitly, although there really is no need to do so:
  unless ( lock_store( $status, $STATUS_FILE) ) {
      dlog('error',
           'unable to store status information',
           { storable => $STATUS_FILE });
  }
  unlock_pid_file($GLOBAL_LOCK);
  dlog('info',
       'exiting',
       {'total_run_time_seconds' => time()-$RUNTIME});
}

sub system_runner {
    local %logparams = %logparams;
    local %ENV = %ENV;
    my $prefix = uc $APP_NAME;
    $prefix =~ s/\-/_/g;

    my $tag = shift;
    $logparams{cmd} = shift;

    my %pairs = ('BACKUP_LOCATION' => $config->backup_location,
                 'LOG_LOCATION'    => $config->log_location,
                 'TMPDIR'          => $config->tmpdir,
                 'COMPRESSION'     => $config->compression,
                 'COMPRESS'        => $compressors{compress},
                 'UNCOMPRESS'      => $compressors{uncompress},
                 'SUFFIX'          => $compressors{suffix},
             );
    while ( my ($k,$v) = each %pairs) {
        $ENV{join('_',$prefix, uc $k)} = $v;
    }

    unless( POSIX::WIFEXITED(system( $logparams{cmd} ) ) ) {
        $logparams{errno} = "${ERRNO}";
        dlog( 'error', $tag, \%logparams );
        return;
    }
    $logparams{exit} = POSIX::WEXITSTATUS($CHILD_ERROR);
    unless ( $logparams{exit} == 0 ) {
        dlog( 'error', $tag, \%logparams );
        return;
    }
    dlog( 'debug', $tag, \%logparams );
    return 1;
}

sub parse_config {
  ## Configuration file parsing:
  # parse --config command line options, removing it from @ARGV
  GetOptions('config_file|config|config-file=s' => \$CONFIG_FILE,
             'restore'                          => \$RESTORE,
            );
  # read the config file
  if ( -f $CONFIG_FILE ) {
    $config->file($CONFIG_FILE);
  } else {
    warn "configuration file does not exist (${CONFIG_FILE}), proceeding from command line options only";
  }
  ## Command Line Parsing:
  $config->getopt;
  # if there is anything left, we must acquit:
  if (@ARGV > 0) {
    die "unparsed remaining command line options: @{ARGV}";
  }

  # print usage if request:
  pod2usage(-1) if $config->help;
}

# pass an array of elemeents to join, and an option arrayref of
# suffixes:
sub backup_file_name_builder {
    my $sufs = pop;
    unless( ref $sufs eq 'ARRAY' ) {
        push @_, $sufs;
        undef $sufs;
    }
    # two array refs
    return join('-',@_).($sufs ? '.'.join('.', @{$sufs}) : '');
}

# called perform backup, however it is mostly concerned with what
# happens after the backup/dump is performed
sub perform_backup {
    my $databasetype = shift;
    my $database = shift;
    local %logparams = (
        databasetype    => $databasetype_case{$databasetype},
        compression     => $config->compression,
        backup_location => $config->backup_location,
        databasename    => $database,
    );
    # record the time for logging:
    my $start_time = time();
    # this is the directory we intend to write the dump to:
    my $current_dir = File::Spec->catfile($config->backup_location,'current');
    # suffix or not:
    my @suffix = ( 'sql' ) ;
    unless ($config->compression eq 'none' ) {
        push @suffix, $compressors{$config->compression}{'extension'};
    }
    # the components of the file name to eventually create:
    my @file_parts = ( $databasetype, $database );
    my $file_name = backup_file_name_builder(@file_parts,\@suffix);
    # the backup path is the full path to the database dump we are
    # trying to create:
    my $backup_path = File::Spec->catfile($current_dir,$file_name);
    $logparams{backup_file} = $backup_path;
    # create a useful list of alternate possible last backups:
    my @alternates = path_alternates($backup_path);
    # output_path is a full path to a temporary file to initially dump
    # the database into:
    my ($_of,$output_path) = tempfile("${file_name}.XXXXXXXX",
                                      DIR => $config->tmpdir ? $config->tmpdir : $current_dir);
    # this timestamp should be used in the creation of the symlinks:
    my $backup_timestamp = strftime "%FT%T",  localtime($start_time);
    my $file_name_timestamp = backup_file_name_builder(@file_parts,$backup_timestamp,\@suffix);

    # could probably use a function reference here:
    my $return_value = 0;
    if ( $databasetype eq 'mysql' ) {
        $return_value = mysql_dump($database,$output_path);
    }
    elsif ( $databasetype eq 'postgresql' ) {
        $return_value = postgresql_dump($database,$output_path);
    }
    elsif ( $databasetype eq 'mongodb' ) {
        $return_value = mongodb_dump($database,$output_path);
    }
    else {
        my $dmsg = "unknown database type: ${databasetype}";
        dlog('critical',$dmsg, \%logparams);
        die $dmsg;
    }

    if ($return_value and $return_value == 1) { # $return_value MUST be 1, otherwise the first clause should match
      # 1 is happy
      # this is the relative link path to the current backup in current:
      my $relative_name = File::Spec->catfile('..','current',$file_name);

      if($config->localcow and -f $backup_path) {
        local %logparams = %logparams; # is this a copy???
        my ($_tf,$tf_path) = tempfile("${file_name}.XXXXXXXX",
                                      DIR => $config->backup_location);
        $logparams{temp_file} = $tf_path;
        unless ( rsync($backup_path,$tf_path) ) {
            dlog('error', "unable to rsync from backup to location to temp file", \%logparams);
            unlink $tf_path;
            return;
        }
        link_symlinks(backup_file_name_builder(@file_parts).'*',
                      $relative_name,
                      $tf_path);
        unlink $tf_path;
      }
      else {
        # look in each directory for symlinks to the current file:
          link_symlinks(backup_file_name_builder(@file_parts).'*',
                        $relative_name,
                        $backup_path);
      }

      # remove the old output file, now that the hard links will
      # preserve it:
      foreach my $alt (path_alternates($backup_path)) {
          # delete all the current files for this backup, unless it's
          # the one we intend to write in this pass and localcow is
          # enabled:
          next unless -f $alt;
          next if ( $config->localcow and $backup_path eq $alt );
          unlink $alt;
      }

      # rename our temp file into the file we're trying to create:
      my $rename_result=0;
      unless($config->localcow) {
        if(rename $output_path, $backup_path) {
          $rename_result=1;
        }
        elsif( $! eq 'Invalid cross-device link' ) {
          # this is ok to continue, we can just use rsync later
          $rename_result=0;
        }
        else {
          local %logparams = %logparams; # is this a copy???
          $logparams{errno} = $!;
          dlog('error', 'rename error', \%logparams);
          unlink $output_path;
          return;
        }
      }
      if($rename_result == 0 or $config->localcow) {
          unless ( rsync($output_path,$backup_path) ) {
              local %logparams = %logparams;
              $logparams{backup_path} = $backup_path;
              $logparams{output_path} = $output_path;
              dlog('error', "unable to rsync from output path to backup location", \%logparams);
              unlink $output_path if -f $output_path;
              return;
          }
          unlink $output_path if -f $output_path;
      }
      # create a series of symlinks to this path...
      # we do this to keep rsync from having to work so hard
      # preserving the hard links:
      symlink $relative_name,
        File::Spec->catfile( $config->backup_location,
                             'daily',
                             $file_name_timestamp);
      if ( strftime( "%a", localtime($start_time) ) eq $config->week_start ) {
        symlink $relative_name,
          File::Spec->catfile( $config->backup_location,
                               'weekly',
                               $file_name_timestamp);
      }
      if ( strftime( "%d", localtime($start_time) ) eq sprintf('%02d', $config->month_start) ) {
        symlink $relative_name,
          File::Spec->catfile( $config->backup_location,
                               'monthly',
                               $file_name_timestamp);
      }
      dlog('notice',
           "backup status",
           \%logparams);
      clean_old_dumps($config->backup_location, $databasetype, $database);
      return $start_time;
  }
  else {
      # presumed to have failed... I think we need to clean up manually
      dlog('info','perform cleanup', \%logparams);
      unlink $output_path if -f $output_path;
  }
  dlog('error','perform backup fall through failure', \%logparams);
  return;
}

# use daily/weekly/monthly values to remove files
sub clean_old_dumps {
    my $path = shift;
    my $databasetype = shift;
    my $databasename = shift;
    local %logparams = %logparams;

 PERIOD:
    foreach my $period (@periods) {
        $logparams{'period'} = $period;
        $logparams{'dir'} = File::Spec->catfile($path,$period);

        my $n = $config->get($period) ? $config->get($period) : 0;
        $n = $n * 7 if $period eq 'weekly';
        $n = $n * 31 if $period eq 'monthly';
        $n *= 86400; # convert to seconds

    FILE:
        foreach my $file (glob(File::Spec->catfile($logparams{'dir'},"${databasetype}-${databasename}*")),
                          glob(File::Spec->catfile($path,            "${databasename}.${period}.sql.*")),
                      ) {
            next if -l $file; # explicitly skip symlinks
            next unless -f $file; # do not act on non-regular files
            $logparams{'file'} = $file;
            my $timestamp;

            if ( my ($i) = basename($file) =~ m{(\d\d\d\d . \d\d . \d\d T \d\d . \d\d . \d\d)}xms ) {
                $timestamp = str2time($i);
            }
            elsif ( my ($year,$month,$day,$hour,$minute) = basename($file) =~ m{(\d\d\d\d) . (\d\d) . (\d\d) . (\d\d) . (\d\d)}xms ) {
                $timestamp = mktime(0,
                                    $minute,
                                    $hour,
                                    $day,
                                    ($month - 1),
                                    ($year - 1900)
                                );
            }
            else {
                dlog( 'error', "unable to parse time stamp from file", \%logparams );
                next FILE;
            }
            $logparams{file_timestamp} = $timestamp;
            $logparams{file_datetime} = localtime($timestamp);
            if( (time() - $timestamp) > $n ) {
                if( unlink $file ) {
                    dlog('info','unlink dump', \%logparams );
                }
                else {
                    dlog('error','unlink dump', \%logparams );
                }
            }
        }
    }
}

sub path_alternates {
    my $p = shift;
    my @suffixes
        = map { '.' . $compressors{$_}{extension}; } ( keys(%compressors) );

    # create a list of possible alternative backup paths (if compression changes)
    my @alternates = map {
        my ( $f, $d, $s ) = fileparse( $p, @suffixes );
        File::Spec->catfile( $d, $f . $_ );
    } @suffixes;
    my ( $f, $d, $s ) = fileparse( $p, @suffixes );
    push @alternates, File::Spec->catfile( $d, $f );
    return @alternates;
}

sub verify_mysql_backup {
    local %logparams = %logparams;

    my $backup_path = shift;
    $logparams{backup_path} = $backup_path;
    my @pipes = (
        { 'stage' => 'tail', 'exec' => [ 'tail', '-v', '-n', '1' ] },
        {   'stage' => 'grep',
            'exec'  => [ 'grep', '-q', 'Dump completed' ]
        },
    );
    if ( $config->compression ne 'none' ) {
        unshift(@pipes,
                {
                    'stage' => 'uncompress',
                    'exec' => [split(/\s+/,$compressors{$config->compression}{'uncompress'})],
                }
            );
    }
    my $start_time = time();
    my $exit_map = pipeline(\@pipes, $backup_path);
    $logparams{runtime} = time() - $start_time;
    $logparams{exit} = 0;
 STAGE:
    foreach my $hr (@pipes) {
        my $stage = $$hr{stage};
        if (exists $$exit_map{$stage} and $$exit_map{$stage} != 0) {
            $logparams{failed_stage} = $stage;
            $logparams{exit} = $$exit_map{$stage};
            last STAGE;
        }
    }

    if ( $logparams{exit} == 0 ) {    # check the return code
        dlog('info', 'dump verify', \%logparams);
        return 1;
    }
    dlog('error', 'dump verify', \%logparams);
    return;
}

sub mysql_dump {
  my $database = shift;
  my $backup_path = shift;
  local %logparams = %logparams;
  $logparams{backup_path} = $backup_path;

  my @mysql_exec = build_mysql_exec('mysqldump');
  push(@mysql_exec, $database);

  my @pipelines = ( { 'stage' => 'dump', 'exec' => \@mysql_exec } );

  unless ( $config->compression eq 'none' ) {
      push(@pipelines,{
          'stage' => 'compress',
          'exec' => [
              split(/\s+/,${compressors{$config->compression}{compress}}),
          ],
      });
  }
  my $start_time = time();
  my $exit_map = pipeline(
      \@pipelines,
      undef,
      $backup_path,
  );
  $logparams{runtime} = time() - $start_time;
  $logparams{exit} = 0;
 STAGE:
  foreach my $hr (@pipelines) {
      my $stage = $$hr{stage};
      if (exists $$exit_map{$stage} and $$exit_map{$stage} != 0) {
          $logparams{exit} = $$exit_map{$stage};
          $logparams{failed_stage} = $stage;
          last STAGE;
      }
  }

  if ( $logparams{exit} != 0 or not verify_mysql_backup($backup_path)) {
      dlog('error','dump status',\%logparams);
      return;
  }
  else {
    dlog('info','dump status',\%logparams);
    return 1;
  }
}

sub mongodb_dump {
  my $database = shift;
  my $backup_path = shift;
  local %logparams = %logparams;
  $logparams{backup_path} = $backup_path;

  my @exec = build_mongodb_exec();

  my @pipelines = ( { 'stage' => 'dump', 'exec' => \@exec } );

  unless ( $config->compression eq 'none' ) {
      push(@pipelines,{
          'stage' => 'compress',
          'exec' => [
              split(/\s+/,${compressors{$config->compression}{compress}}),
          ],
      });
  }
  my $start_time = time();
  my $exit_map = pipeline(
      \@pipelines,
      undef,
      $backup_path,
  );
  $logparams{runtime} = time() - $start_time;
  $logparams{exit} = 0;
 STAGE:
  foreach my $hr (@pipelines) {
      my $stage = $$hr{stage};
      if (exists $$exit_map{$stage} and $$exit_map{$stage} != 0) {
          $logparams{exit} = $$exit_map{$stage};
          $logparams{failed_stage} = $stage;
          last STAGE;
      }
  }

  if ( $logparams{exit} != 0 ) {
      dlog('error','dump status',\%logparams);
      return;
  }
  else {
    dlog('info','dump status',\%logparams);
    return 1;
  }
}

sub pipeline {
    my @pipes = @{ shift(@_) };

    # pipes should look like this:
    # { 'stage' => 'compress', 'exec' => ['bzip2','-c'] },
    # etc.
    my $initial_input = shift;
    my $input_handle;
    my $final_output = shift;    # file to write to
    my $output_handle;
    my %child_map;               # map pids to stage
    my %exit_map;                #map stages to exit codes
    local %logparams = %logparams;
    my @stages;

    if ($initial_input) {
        unless(open($input_handle, '<', $initial_input)) {
            $logparams{file} = $initial_input;
            $logparams{errno} = $ERRNO;
            dlog('error','unable to open input file',\%logparams);
            die 'unable to open input file: '.$logparams{file};
        }
    }
    if ($final_output) {
        unless(open($output_handle, '>', $final_output)) {
            $logparams{file} = $final_output;
            $logparams{errno} = $ERRNO;
            dlog('error','unable to open output file',\%logparams);
            die 'unable to open output file: '.$logparams{file};
        }
    }

    for my $pos (0..$#pipes) {
        my ($input,$output);

        if ($pos == $#pipes) { # we are the last iteration!
            # also possible the only iteration?
            ($input,$output) = ( $input_handle, $output_handle );
        }
        else {
            my ($pr);
            unless(pipe($pr,$output)) {
                $logparams{errno} = $ERRNO;
                dlog('error','unable to call pipe()',\%logparams);
                die 'unable to call pipe()';
            }
            # last interations read bits
            $input = $input_handle;
            # save the read end of this
            $input_handle = $pr;
        }
        $child_map{split_fork_exec($input,
                                   $output,
                                   ( defined $pipes[$pos]{'user'} ? $pipes[$pos]{'user'} : undef ),
                                   @{$pipes[$pos]{'exec'}}),
               } = $pipes[$pos]{'stage'};
    }

    my $MAXWAIT=1;
    do {
        $MAXWAIT = $MAXWAIT * 2;
        while ( (my $pid = waitpid(-1, WNOHANG)) > 0 ) {
            dlog('debug',"waitpid in while loop returned child ${pid}",\%child_map);
            if(exists $child_map{$pid}) {
                $exit_map{$child_map{$pid}} = POSIX::WEXITSTATUS(${^CHILD_ERROR_NATIVE});
                dlog( 'debug',
                      "waitpid returned child ${pid} with exit: ".$exit_map{ $child_map{$pid} },
                      \%exit_map );
                delete $child_map{$pid};
                $MAXWAIT=1;
            }
            else {
                dlog( 'error', "waitpid returned child ${pid} not in map",
                      \%exit_map );
            }
        }
        if (scalar(keys(%child_map)) > 0) {
            # set some signal handlers so that we can awak from pause
            local $SIG{CHLD} = sub { };
            local $SIG{ALRM} = sub { };
            dlog('debug',
                 'waiting on '.(scalar(keys(%child_map))).' sub-processes to exit, pausing',
                 \%logparams);
            my $pause_start=time();
            alarm $MAXWAIT;
            pause;
            alarm 0;
            dlog('debug',
                 "paused for: ".sprintf('%.5f seconds',time()-$pause_start),
                 \%logparams);
        };
    } until( scalar(keys(%child_map)) == 0 );
    return(\%exit_map);
}

sub split_fork_exec {
    my $pr = shift;
    my $pw = shift;
    my $user = shift;

    my $pid = fork();
    # I don't think logging is worth anything at this point:
    die "cannot fork" unless defined $pid;

    if ($pid == 0) {
        # child needs to do some stuff:
        if($user) {
            POSIX::setuid(scalar getpwnam($user));
            if ($! != 0) {
                die "cannot setuid as specified to ${user}";
            }
        }
        if ($pr) {
            unless(open(STDIN, '<&', $pr)) {
                die 'cannot duplicate filehandle into STDIN';
            }
        }
        else {
            close STDIN;
        }
        if ($pw) {
            #close STDOUT;
            # dup2(0,$pw) or die $!;
            unless(open(STDOUT, '+<&', $pw)) {
                die 'cannot duplicate filehandle into STDOUT';
            }
        }
        local %logparams = %logparams;
        $logparams{exec} = join(' ',@_);

        dlog('debug','split_fork_exec',\%logparams);
        exec(@_) or die 'unable to exec: '.join(' ',@_);
    }
    else {
        close $pr if $pr;
        close $pw if $pw;
        return $pid;
    }
}

sub postgresql_dump {
    my $database = shift;
    my $backup_path = shift;
    my $backup_handle;
    local %logparams = %logparams;
    $logparams{backup_path} = $backup_path;

    my @pg_exec = build_postgresql_exec( $database eq 'all' ? 'pg_dumpall' : 'pg_dump' );
    if ( $database ne 'all' ) {
        push(@pg_exec,$database);
    }

    my @pipelines = ( { 'stage' => 'dump', 'exec' => \@pg_exec, 'user' => 'postgres' } );
    unless ( $config->compression eq 'none' ) {
        push(@pipelines,{
            'stage' => 'compress',
            'exec' => [
                split(/\s+/,${compressors{$config->compression}{compress}}),
            ],
        });
    }

    my $start_time = time();
    my $exit_map = pipeline(
        \@pipelines,
        undef,
        $backup_path,
    );
    $logparams{runtime} = time() - $start_time;
    $logparams{exit} = 0;

 STAGE:
    foreach my $hr (@pipelines) {
        my $stage = $$hr{stage};
        if ( exists $$exit_map{$stage} and $$exit_map{$stage} != 0 ) {
            $logparams{exit}         = $$exit_map{$stage};
            $logparams{failed_stage} = $stage;
            last STAGE;
        }
    }

    if ( $logparams{exit} != 0 ) {
        dlog( 'error', 'dump status', \%logparams );
        return;
    }
    else {
        dlog( 'info', 'dump status', \%logparams );
        return 1;
    }

}

sub build_postgresql_exec {
    my $bin = shift;
    my @com = ($config->postgresql_bindir ? File::Spec->catfile($config->postgresql_bindir,$bin) : $bin);

    if( $config->postgresql_extra_option and scalar @{$config->postgresql_extra_option} > 0 ) {
        push(@com, @{$config->postgresql_extra_option});
    }

    if ($config->postgresql_username) {
        push(@com, '--username='.$config->postgresql_username);
    }
    if ($config->postgresql_host) {
        push(@com, '--host='.$config->postgresql_host);
    }
    return @com;
}


sub sanity_check_config {
  unless ($config->mysql or $config->postgresql) {
    my $emsg='neither mysql nor postgresql was specified, nothing to dump!';
    dlog('error',$emsg,{});
    die $emsg;
  }
  if ($config->compression eq 'custom') {
    $compressors{'custom'} = {};
    foreach my $p (qw( compress uncompress extension )) {
      if ($config->get($p)) {
        $compressors{'custom'}{$p} = $config->get($p);
      }
      else {
        dlog('error',"when using custom compression, you must specify ${p} parameter");
        die "when using custom compression, you must specify ${p} parameter";
      }
    }
  }
}

sub link_symlinks {
  my $pat = shift;
  my $relative_name = shift;
  my $backup_path = shift;
  my @alternates = path_alternates($relative_name);
  foreach my $p (@periods) {
    my $ddir = File::Spec->catfile($config->backup_location,$p);
    foreach my $dbf (glob(File::Spec->catfile($ddir,$pat))) {
      # if we find a relative link, turn it into a hard link before the file goes away:
      if (-l $dbf and string_any(readlink($dbf),@alternates)) {
        # remove this link whether the target exists or not, if
        # the target of a link is missing it's useless anyway:
        unlink($dbf)
            or dlog('error', "unable to unlink ${dbf}", \%logparams);
        # create a hard link to a path that's about to disappear:
        if ( -f $backup_path ) {
            if ( link($backup_path, $dbf) ) {
                dlog('debug', "created link ${backup_path} <- ${dbf}", \%logparams);
            }
            else {
                local %logparams = %logparams;
                $logparams{errno} = $ERRNO;
                dlog('error', "unable to link ${backup_path} <- ${dbf}", \%logparams);
            }
        }
      }
    }
  }
}

sub rsync {
    my ( $src, $dst ) = @_;
    local %logparams = %logparams;
    $logparams{src} = $src;
    $logparams{dst} = $dst;

    return system_runner(
        'rsync copy',
        sprintf( '%s %s %s %s',
            $config->rsync_binary, $config->rsync_options, $src, $dst )
    );
}

# copied from the old version of List::Util:
sub string_any {
    my $s = shift;
    foreach (@_) {
        return 1 if $s eq $_;
    }
    return 0;
}

=head1 NAME

delta-dumper - wrapper for mysqldump, pg_dump, and xdelta

=head1 SYNOPSIS

B<delta-dumper> {B<-h|--help>} {B<--mysql|--no-mysql>} {B<--postgresql|--no-postgresql>}

=head1 DESCRIPTION

Parse a config file and command line, invoke various dump programs.

=head1 OPTIONS

=head2 MAJOR MODES

delta-dumper's default mode is to dump databases and write them to the configured location.

=over 4

=item B<--restore>



=back

=head2 CONFIGURATION OPTIONS

=over 4

=item B<--mysql|--no-mysql> C<mysql>

Default: false

Perform (or don't) mysql database dumps.

=item B<--postgresql|--no-postgresql> C<postgresql>

Default: false

Perform (or don't) postgresql database dumps.

=item B<--mongodb|--no-mongodb> C<mongodb>

Default: false

Perform (or don't) mongodb database dumps.

=item B<--tmpdir>=I<dir> C<tmpdir>

Directory used to write the output of dump commands.  The default is
to use the 'current' directory in the C<backup_location> directory.

=item B<--localcow|--no-localcow> C<localcow> (Boolean)

When localcow is set, we take great pains to ensure that we update the
dump files with the latest changes using rsync, in hopes of minimizing
the change in the file and therefore the amount of data used to
capture those changes in a snapshot.

localcow should be set when the backup_location is on a filesystem
with cow or cow-like characteristics, ie: btrfs, zfs, netapp, etc.

=item B<--postrun>=I<command> C<postrun>

Run this command after all (not each) dumps.

=item B<--prerun>=I<command> C<prerun>

Run this command before all (not each) dumps.

=back

=head2 LOGGING OPTIONS

=over 4

=item B<--file_logging|--no-file_logging> C<file_logging>

Default: true

Enable or disable logging to configured log location.  The minimum
severity (level) logged to the file is controlled by the log_level
parameter.

=item B<--sys_logging|--syslog|--no-sys_logging|--no-syslog> C<sys_logging>

Default: true

Enable or disable logging syslog.  The minimum severity (level) logged
to the file is controlled by the log_level parameter and the facility
used is configured with the paramater of the same name.

=item B<--terminal_logging|-t|--no-terminal_logging|--no-t> C<terminal_logging>

Default: true if stdout is a terminal, false otherwise.

Enable or disable logging to stderr.  The minimum severity (level)
logged to stderr is controlled by the log_level parameter.

=item B<--log_location> C<log_location> (String)

Set the directory for logging.  Default for root is /var/log and the
default for non-root users is ~/var/log.

=item B<--log_level|--level|--log-level|--loglevel> C<log_level> (String)

Default: info

Log messags of this severity and above will be written to the various
logging outputs, if enabled.  These correspond to the long names of
the syslog severities:

C<^(debug|info|notice|warning|error|critical|alert|emergency)$>

=item B<--facility> C<facility> (String)

Default: user

Use this facility when logging to syslog which must match:

C<^(auth|authpriv|cron|daemon|kern|local[0-7]|mail|news|syslog|user|uucp)$>

=back

=head2 COMPRESSION OPTIONS

=over 4

=item B<--compression> C<compresion> (String)

Default: gzip

Use pre-configured compression commands, gzip, bzip2, xz, and zstd.

Also, C<none> is accepted along with C<custom> which requires setting
the compress, uncompress, and suffix configuration items.

=item B<--compress> C<compress> (String)

Compress command to use when compression is set to custom.  Must read
from a file and write to stdout.

IE: C<gzip --stdout>

=item B<--uncompress> C<uncompress> (String)

Uncompress command to use when compression is set to custom.  Must read
from a file and write to stdout.

IE: C<gzcat>

=item B<--suffix|--extension> C<extension> (String)

Suffix to use when creating files when compression is set to custom.
Must read from a file and write to stdout. Do not specify the period
before the suffix.

IE: C<gz>

=back

=head2 MYSQL OPTIONS

=over 4

=item B<--mysql_bindir>=I<dir> C<mysql_bindir>

If specified, use this directory to create the path to the mysql and
mysqldump executables.  Otherwise, assume they are in the path.

=item B<--mysql_defaults_file|--mysql_defaults-file|--defaults-file>=I<file>

When invoking mysql and mysqldump, call them with --defaults-file and
the value of this parameter.

=item B<--mysql_single_transaction|--no-mysql_single_transaction|--single-transaction|--no-single-transaction> (Boolean)

Default: true

Pass --single-transaction to mysqldump, or not.

=item B<--mysql_ignore_table>=I<table>

Default: mysql.events

Can be specified multiple times, each of which will be passed to the
--ignore-table option of mysqldump.

=item B<--mysql_user>=I<user>

Passed to -u on mysql and mysqldump commands.

=item B<--mysql_password>=I<password>

Passed to -p on mysql and mysqldump commands.

=item B<--mysql_hostname>=I<hostname>

Passed to -h on mysql and mysqldump commands.

=item B<--mysql_extra_option>=I<option> C<mysql_extra_option>

Can be specified multiple times, options passed to mysqldump, if set.

=back

=head2 POSTGRESQL OPTIONS

=over 4

=item B<--postgresql_bindir>=I<dir> C<postgresql_bindir>

If specified, use this directory to create the path to the postgresql
dump executables.  Otherwise, assume they are in the path.

=item B<--postgresql_dump_database>=I<db> C<postgresql_dump_database>

List of databases to dump, can be specified multiple timees on the
command line and in the config file.

=item B<--postgresql_dump_all> C<postgresql_dump_all>

Default: true

If true, dump all.  If false assume that some list of databases was
specified with the postgresql_dump_database directive.

=item B<--postgresql_extra_option>=I<option> C<postgresql_extra_option>

Default: --clean --if-exists

Can be specified multiple times, options passed to dump or dump all.

=item B<--postgresql_username>=I<username> C<postgresql_username>

Default: postgres

Passed to the --username= option of the dump programs.

=item B<--postgresql_host>=I<hostname> C<postgresql_host>

Passed to the --host= option of the dump programs.  If unset, don't
pass --host at all.

=back

=head2 MONGODB OPTIONS

=over 4

=item B<--mongodb_bindir>=I<dir> C<mongodb_bindir>

If specified, use this directory to create the path to the mongodb
dump executable.  Otherwise, assume they are in the path.

=item B<--mongodb_username>=I<username> C<mongodb_username>

Passed to the --username= option of the dump programs.

=item B<--mongodb_password>=I<password>

Passed to --password on mongodump command.

=item B<--mongodb_port>=I<port>

Passed to --port on mongodump command.

=item B<--mongodb_hostname>=I<hostname>

Passed to --hostname on mongodump command.

=item B<--mongodb_ssl|--no-mongodb_ssl>

Default: true

If true, pass --ssl to mongodump.

=back

=head2 RETENTION OPTIONS

=over 4

=item B<--daily>=I<count>

Keep <count> days worth of daily dumps.

=item B<--weekly>=I<count>

Keep <count> weeks worth of weekly dumps.

=item B<--monthly>=I<count>

Keep <count> months worth of monthly dumps.

=item B<--week_start>=I<day>

Default: Sun

When the day matches this string (Sun, Mon, etc.) then a weekly backup
will be created in addition to the daily.

=item B<--month_start>=I<day>

Default: 1

When the month day matches this a monthly backup will be created in
addition to the daily.

=back

=head2 RSYNC OPTIONS

Rsync is used when rename won't work, which is when the temporary
database file is across a filesystem boundary.

=over 4

=item B<--rsync_binary>

DEFAULT: rsync

Explicit path to rsync binary, if needed.

=item B<--rsync_options>=I<option>

DEFAULT: -a --inplace --no-whole-file

Options to pass to rsync when copying files.

=back

=head1 RETURN VALUE

=head1 ERRORS

=head1 DIAGNOSTICS

=head1 EXAMPLES

=head1 ENVIRONMENT

=head1 FILES

=head2 F</etc/delta-dumper/config>

Default config file when run as root.

=head2 F<$HOME/.config/delta-dumper/config>

Default config file when run as non-root.

=head1 CAVEATS

=head1 BUGS

=head1 RESTRICTIONS

=head1 NOTES

=head1 AUTHOR

Aran Cox <arancox@gmail.com>

=head1 HISTORY

=head1 SEE ALSO

=over 4

gzip(1), bzip2(1), zstd(1), xdelta(1)

=back
